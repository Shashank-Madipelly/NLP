{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_IDF_cosine_similarities.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAXsNlTVzel3"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "MmNsIZl28LAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentA = 'Text Simplification is the task of reducing the complexity of the vocabulary and sentence structure of text while retaining its original meaning, with the goal of improving readability and understanding.'\n",
        "documentB = 'Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. A sentiment analysis system for text analysis combines natural language processing (NLP) and machine learning techniques to assign weighted sentiment scores to the entities, topics, themes and categories within a sentence or phrase.'"
      ],
      "metadata": {
        "id": "RvtRW5tEz-Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bagOfWordsA = documentA.split(' ')\n",
        "bagOfWordsB = documentB.split(' ')"
      ],
      "metadata": {
        "id": "g4Nz-IU50P1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
      ],
      "metadata": {
        "id": "nWROh6Rl0VAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsA:\n",
        "    numOfWordsA[word] += 1\n",
        "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsB:\n",
        "    numOfWordsB[word] += 1"
      ],
      "metadata": {
        "id": "iYgFBct80Yec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "#stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y464OoO0daS",
        "outputId": "850baeaf-9e67-4f2b-93f6-57739418b607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency (TF) \n",
        "The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency."
      ],
      "metadata": {
        "id": "Rtmoly0d1StP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bagOfWordsCount)\n",
        "    return tfDict"
      ],
      "metadata": {
        "id": "zQHLiXIj2Es4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
        "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
        "print(tfA)\n",
        "print(tfB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvduQkZi2WTY",
        "outputId": "b9e803f7-ecab-49b6-a87b-66b225f8ed96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'determining': 0.0, 'a': 0.0, 'meaning,': 0.03333333333333333, 'readability': 0.03333333333333333, 'vocabulary': 0.03333333333333333, 'analysis': 0.0, 'Simplification': 0.03333333333333333, 'improving': 0.03333333333333333, 'goal': 0.03333333333333333, 'language': 0.0, 'assign': 0.0, 'the': 0.13333333333333333, 'Text': 0.03333333333333333, 'while': 0.03333333333333333, 'text': 0.03333333333333333, 'its': 0.03333333333333333, 'system': 0.0, 'writing': 0.0, 'scores': 0.0, 'positive,': 0.0, 'themes': 0.0, 'is': 0.03333333333333333, 'retaining': 0.03333333333333333, 'phrase.': 0.0, 'complexity': 0.03333333333333333, 'understanding.': 0.03333333333333333, 'original': 0.03333333333333333, 'of': 0.13333333333333333, 'negative': 0.0, 'sentence': 0.03333333333333333, 'Sentiment': 0.0, 'process': 0.0, 'structure': 0.03333333333333333, 'whether': 0.0, 'machine': 0.0, 'learning': 0.0, 'with': 0.03333333333333333, 'neutral.': 0.0, 'for': 0.0, 'task': 0.03333333333333333, 'sentiment': 0.0, 'processing': 0.0, 'and': 0.06666666666666667, 'categories': 0.0, 'A': 0.0, 'techniques': 0.0, 'reducing': 0.03333333333333333, '(NLP)': 0.0, 'within': 0.0, 'or': 0.0, 'to': 0.0, 'entities,': 0.0, 'natural': 0.0, 'weighted': 0.0, 'piece': 0.0, 'topics,': 0.0, 'combines': 0.0, 'Analysis': 0.0}\n",
            "{'determining': 0.02, 'a': 0.04, 'meaning,': 0.0, 'readability': 0.0, 'vocabulary': 0.0, 'analysis': 0.04, 'Simplification': 0.0, 'improving': 0.0, 'goal': 0.0, 'language': 0.02, 'assign': 0.02, 'the': 0.04, 'Text': 0.0, 'while': 0.0, 'text': 0.02, 'its': 0.0, 'system': 0.02, 'writing': 0.02, 'scores': 0.02, 'positive,': 0.02, 'themes': 0.02, 'is': 0.04, 'retaining': 0.0, 'phrase.': 0.02, 'complexity': 0.0, 'understanding.': 0.0, 'original': 0.0, 'of': 0.04, 'negative': 0.02, 'sentence': 0.02, 'Sentiment': 0.02, 'process': 0.02, 'structure': 0.0, 'whether': 0.02, 'machine': 0.02, 'learning': 0.02, 'with': 0.0, 'neutral.': 0.02, 'for': 0.02, 'task': 0.0, 'sentiment': 0.04, 'processing': 0.02, 'and': 0.04, 'categories': 0.02, 'A': 0.02, 'techniques': 0.02, 'reducing': 0.0, '(NLP)': 0.02, 'within': 0.02, 'or': 0.04, 'to': 0.04, 'entities,': 0.02, 'natural': 0.02, 'weighted': 0.02, 'piece': 0.02, 'topics,': 0.02, 'combines': 0.02, 'Analysis': 0.02}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inverse Data Frequency (IDF)**\n",
        "The log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus. "
      ],
      "metadata": {
        "id": "y0xnLIuQ2-Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeIDF(documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict"
      ],
      "metadata": {
        "id": "MCHvwv4-3Cem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
        "print(idfs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz9l1xlw20pU",
        "outputId": "b251bbcc-300b-4e94-98f0-f966ce93f6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'determining': 0.6931471805599453, 'a': 0.6931471805599453, 'meaning,': 0.6931471805599453, 'readability': 0.6931471805599453, 'vocabulary': 0.6931471805599453, 'analysis': 0.6931471805599453, 'Simplification': 0.6931471805599453, 'improving': 0.6931471805599453, 'goal': 0.6931471805599453, 'language': 0.6931471805599453, 'assign': 0.6931471805599453, 'the': 0.0, 'Text': 0.6931471805599453, 'while': 0.6931471805599453, 'text': 0.0, 'its': 0.6931471805599453, 'system': 0.6931471805599453, 'writing': 0.6931471805599453, 'scores': 0.6931471805599453, 'positive,': 0.6931471805599453, 'themes': 0.6931471805599453, 'is': 0.0, 'retaining': 0.6931471805599453, 'phrase.': 0.6931471805599453, 'complexity': 0.6931471805599453, 'understanding.': 0.6931471805599453, 'original': 0.6931471805599453, 'of': 0.0, 'negative': 0.6931471805599453, 'sentence': 0.0, 'Sentiment': 0.6931471805599453, 'process': 0.6931471805599453, 'structure': 0.6931471805599453, 'whether': 0.6931471805599453, 'machine': 0.6931471805599453, 'learning': 0.6931471805599453, 'with': 0.6931471805599453, 'neutral.': 0.6931471805599453, 'for': 0.6931471805599453, 'task': 0.6931471805599453, 'sentiment': 0.6931471805599453, 'processing': 0.6931471805599453, 'and': 0.0, 'categories': 0.6931471805599453, 'A': 0.6931471805599453, 'techniques': 0.6931471805599453, 'reducing': 0.6931471805599453, '(NLP)': 0.6931471805599453, 'within': 0.6931471805599453, 'or': 0.6931471805599453, 'to': 0.6931471805599453, 'entities,': 0.6931471805599453, 'natural': 0.6931471805599453, 'weighted': 0.6931471805599453, 'piece': 0.6931471805599453, 'topics,': 0.6931471805599453, 'combines': 0.6931471805599453, 'Analysis': 0.6931471805599453}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lastly, the TF-IDF is simply the TF multiplied by IDF.**"
      ],
      "metadata": {
        "id": "iqfDkEwN3vMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf"
      ],
      "metadata": {
        "id": "qlinmoWg3yMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)\n",
        "df = pd.DataFrame([tfidfA, tfidfB])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgmGrl8d36n0",
        "outputId": "a7912db5-2f6b-4043-893d-9c9564b7568e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   determining         a  meaning,  readability  vocabulary  analysis  \\\n",
            "0     0.000000  0.000000  0.023105     0.023105    0.023105  0.000000   \n",
            "1     0.013863  0.027726  0.000000     0.000000    0.000000  0.027726   \n",
            "\n",
            "   Simplification  improving      goal  language  ...    within        or  \\\n",
            "0        0.023105   0.023105  0.023105  0.000000  ...  0.000000  0.000000   \n",
            "1        0.000000   0.000000  0.000000  0.013863  ...  0.013863  0.027726   \n",
            "\n",
            "         to  entities,   natural  weighted     piece   topics,  combines  \\\n",
            "0  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.027726   0.013863  0.013863  0.013863  0.013863  0.013863  0.013863   \n",
            "\n",
            "   Analysis  \n",
            "0  0.000000  \n",
            "1  0.013863  \n",
            "\n",
            "[2 rows x 58 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMLWw1u34Efw",
        "outputId": "2b11fb7e-fd5e-458d-a4a9-d6c57deeb307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   analysis       and    assign  categories  combines  complexity  \\\n",
            "0  0.000000  0.233118  0.000000    0.000000  0.000000    0.163819   \n",
            "1  0.380656  0.180560  0.126885    0.126885  0.126885    0.000000   \n",
            "\n",
            "   determining  entities       for      goal  ...       to    topics  \\\n",
            "0     0.000000  0.000000  0.000000  0.163819  ...  0.00000  0.000000   \n",
            "1     0.126885  0.126885  0.126885  0.000000  ...  0.25377  0.126885   \n",
            "\n",
            "   understanding  vocabulary  weighted   whether     while      with  \\\n",
            "0       0.163819    0.163819  0.000000  0.000000  0.163819  0.163819   \n",
            "1       0.000000    0.000000  0.126885  0.126885  0.000000  0.000000   \n",
            "\n",
            "     within   writing  \n",
            "0  0.000000  0.000000  \n",
            "1  0.126885  0.126885  \n",
            "\n",
            "[2 rows x 53 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_model = \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
        "model = SentenceTransformer(_model)"
      ],
      "metadata": {
        "id": "uLSCXrB78XQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isSimilar(a,b):\n",
        "  threshold = 0.9\n",
        "  embeddings = model.encode([a, b])\n",
        "  embeddings.shape\n",
        "  res = list(cosine_similarity([embeddings[0]], embeddings[1:]))\n",
        "  return res[0]\n",
        "print(isSimilar(documentA, documentB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOPh4GyD9YcV",
        "outputId": "8e68c86a-ea32-4f99-e763-9a067afea770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6463746]\n"
          ]
        }
      ]
    }
  ]
}